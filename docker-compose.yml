
services:
  llm-inference:
    image: raha8rah/llm-inference-image
    container_name: llm-inference-compose-container
    ports:
      - "8080:5000"
    environment:
      - METRICS_LOG_FILE=/app/inside_compose_inference_metrics.csv
    volumes:
      - ./compose_inference_metrics.csv:/app/inside_compose_inference_metrics.csv
    #this will make sure the container would restart if a failure occurs
    restart: unless-stopped
